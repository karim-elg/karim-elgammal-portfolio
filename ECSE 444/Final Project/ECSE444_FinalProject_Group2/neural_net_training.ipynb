{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preprocessing\n",
        "In this section, the dataset is put taken from the Google drive, parsed and flattened and finally shuffled so that it will be useful for the neural network. To not overflow Colab's GPU RAM, a portion of the training data is used. "
      ],
      "metadata": {
        "id": "acvidWSd4Y6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put HMP_Dataset in drive to access it within Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBn-p00gh9c7",
        "outputId": "95430a90-0a09-4907-e612-c01e3548b41e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rhwFIKSVaIrd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from random import shuffle\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "from keras.optimizers import Adam, SGD\n",
        "from scipy.signal import medfilt\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import GRU, Input, Reshape\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras import regularizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Source for this block: https://github.com/xtianmcd/accelerometer_rnn_explorations/blob/master/accelerometer_rnn.py\n",
        "\n",
        "def get_filepaths(mainfolder):\n",
        "    \"\"\"\n",
        "    Searches a folder for all unique files and compile a dictionary of their paths.\n",
        "    Parameters\n",
        "    --------------\n",
        "    mainfolder: the filepath for the folder containing the data\n",
        "    Returns\n",
        "    --------------\n",
        "    training_filepaths: file paths to be used for training\n",
        "    testing_filepaths:  file paths to be used for testing\n",
        "    \"\"\"\n",
        "    training_filepaths = {}\n",
        "    testing_filepaths  = {}\n",
        "    folders = os.listdir(mainfolder)\n",
        "    for folder in folders:\n",
        "        fpath = mainfolder + \"/\" + folder\n",
        "        if os.path.isdir(fpath) and \"MODEL\" not in folder:\n",
        "            filenames = os.listdir(fpath)\n",
        "            for filename in filenames[:int(round(0.8*len(filenames)))]:\n",
        "                fullpath = fpath + \"/\" + filename\n",
        "                training_filepaths[fullpath] = folder\n",
        "            for filename1 in filenames[int(round(0.8*len(filenames))):]:\n",
        "                fullpath1 = fpath + \"/\" + filename1\n",
        "                testing_filepaths[fullpath1] = folder\n",
        "    return training_filepaths, testing_filepaths\n",
        "\n",
        "def get_labels(mainfolder):\n",
        "    \"\"\" Creates a dictionary of labels for each unique type of motion \"\"\"\n",
        "    labels = {}\n",
        "    label = 0\n",
        "    for folder in os.listdir(mainfolder):\n",
        "        fpath = mainfolder + \"/\" + folder\n",
        "        if os.path.isdir(fpath) and \"MODEL\" not in folder:\n",
        "            labels[folder] = label\n",
        "            label += 1\n",
        "    return labels\n",
        "\n",
        "def get_data(fp, labels, folders, norm, std, center, med):\n",
        "    \"\"\"\n",
        "    Creates a dataframe for the data in the filepath and creates a one-hot\n",
        "    encoding of the file's label\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(filepath_or_buffer=fp, sep=' ', names = [\"X\", \"Y\", \"Z\"])\n",
        "    if norm and not std:\n",
        "        normed_data = norm_data(data)\n",
        "    elif std and not norm:\n",
        "        stdized_data = std_data(data)\n",
        "    elif center and not norm and not std:\n",
        "        cent_data = subtract_mean(data)\n",
        "    elif med:\n",
        "        normed_data = data\n",
        "        normed_data['X'] = medfilt(data['X'], kernel_size=3)\n",
        "        normed_data['Y'] = medfilt(data['Y'], kernel_size=3)\n",
        "        normed_data['Z'] = medfilt(data['Z'], kernel_size=3)\n",
        "    else:\n",
        "        normed_data = data\n",
        "\n",
        "    one_hot = np.zeros(14)\n",
        "    file_dir = folders[fp]\n",
        "    label = labels[file_dir]\n",
        "    one_hot[label] = 1\n",
        "    return normed_data, one_hot, label\n",
        "\n",
        "# Normalizes the data by removing the mean\n",
        "\n",
        "def subtract_mean(input_data):\n",
        "    # Subtract the mean along each column\n",
        "    centered_data = input_data - input_data.mean()\n",
        "    return centered_data\n",
        "\n",
        "\n",
        "def norm_data(data):\n",
        "    \"\"\"\n",
        "    Normalizes the data.\n",
        "    For normalizing each entry, y = (x - min)/(max - min)\n",
        "    \"\"\"\n",
        "    c_data = subtract_mean(data)\n",
        "    mms = MinMaxScaler()\n",
        "    mms.fit(c_data)\n",
        "    n_data = mms.transform(c_data)\n",
        "    return n_data\n",
        "\n",
        "def standardize(data):\n",
        "    c_data = subtract_mean(data)\n",
        "    std_data = c_data/ pd.std(c_data)\n",
        "    return std_data\n",
        "\n",
        "def vectorize(normed):\n",
        "    \"\"\"\n",
        "    Uses a sliding window to create a list of (randomly-ordered) 300-timestep\n",
        "    sublists for each feature.\n",
        "    \"\"\"\n",
        "    sequences = [normed[i:i+96] for i in range(len(normed)-96)]\n",
        "    shuffle(sequences)\n",
        "    sequences = np.array(sequences)\n",
        "    return sequences\n",
        "\n",
        "def build_inputs(files_list, accel_labels, file_label_dict, norm_bool, std_bool, center_bool, med_bool):\n",
        "    X_seq    = []\n",
        "    y_seq    = []\n",
        "    labels = []\n",
        "    for path in files_list:\n",
        "        normed_data, target, target_label = get_data(path, accel_labels, file_label_dict, norm_bool, std_bool, center_bool, med_bool)\n",
        "        input_list = vectorize(normed_data)\n",
        "        for inputs in range(len(input_list)):\n",
        "            X_seq.append(input_list[inputs])\n",
        "            y_seq.append(list(target))\n",
        "            labels.append(target_label)\n",
        "    X_ = np.array(X_seq)\n",
        "    y_ = np.array(y_seq)\n",
        "    return X_, y_, labels"
      ],
      "metadata": {
        "id": "7_AUaZ5qaVWr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mainpath = \"/content/drive/MyDrive/HMP_Dataset\"\n",
        "\n",
        "activity_labels                  = get_labels(mainpath)\n",
        "training_dict, testing_dict      = get_filepaths(mainpath)\n",
        "training_files                   = list(training_dict.keys())\n",
        "testing_files                    = list(testing_dict.keys())\n",
        "\n",
        "print(activity_labels)\n",
        "\n",
        "# build training inputs and labels\n",
        "X_train, y_train, train_labels = build_inputs(\n",
        "    training_files,\n",
        "    activity_labels,\n",
        "    training_dict,\n",
        "    False, False, False, True)\n",
        "# build tesing inputs and labels\n",
        "X_test, y_test, test_labels    = build_inputs(\n",
        "    training_files,\n",
        "    activity_labels,\n",
        "    training_dict,\n",
        "    False, False, False, True)\n",
        "\n",
        "# alternate train and test datasets without filtering\n",
        "X_train_raw, y_train_raw, train_labels_raw = build_inputs(\n",
        "    training_files,\n",
        "    activity_labels,\n",
        "    training_dict,\n",
        "    False, False, False, False)\n",
        "\n",
        "X_test_raw, y_test_raw, test_labels_raw    = build_inputs(\n",
        "    training_files,\n",
        "    activity_labels,\n",
        "    training_dict,\n",
        "    False, False, False, False)\n"
      ],
      "metadata": {
        "id": "7afhBvvVagBB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d34f9db-4f6a-4781-db6d-5aec1c24605a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Use_telephone': 0, 'Standup_chair': 1, 'Liedown_bed': 2, 'Sitdown_chair': 3, 'Pour_water': 4, 'Walk': 5, 'Drink_glass': 6, 'Descend_stairs': 7, 'Climb_stairs': 8, 'Eat_meat': 9, 'Getup_bed': 10, 'Eat_soup': 11, 'Comb_hair': 12, 'Brush_teeth': 13}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomize the train and test data\n",
        "train_indices = np.random.permutation(len(X_train))\n",
        "X_train = X_train[train_indices]\n",
        "y_train = y_train[train_indices]\n",
        "train_labels = np.array(train_labels)[train_indices].tolist()\n",
        "\n",
        "test_indices = np.random.permutation(len(X_test))\n",
        "X_test = X_test[test_indices]\n",
        "y_test = y_test[test_indices]\n",
        "test_labels = np.array(test_labels)[test_indices].tolist()\n",
        "\n",
        "train_indices_raw = np.random.permutation(len(X_train_raw))\n",
        "X_train_raw = X_train_raw[train_indices_raw]\n",
        "y_train_raw = y_train_raw[train_indices_raw]\n",
        "train_labels_raw = np.array(train_labels_raw)[train_indices_raw].tolist()\n",
        "\n",
        "test_indices_raw = np.random.permutation(len(X_test_raw))\n",
        "X_test_raw = X_test_raw[test_indices_raw]\n",
        "y_test_raw = y_test_raw[test_indices_raw]\n",
        "test_labels_raw = np.array(test_labels_raw)[test_indices_raw].tolist()"
      ],
      "metadata": {
        "id": "bmGx_PZ-9iMw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take one part of the training data so the Colab GPU ram doesn't overflow\n",
        "X_train.shape\n",
        "X_test.shape\n",
        "\n",
        "X_train = X_train[:50000, :, :]\n",
        "X_test = X_test[:50000, :, :]\n",
        "\n",
        "y_train = y_train[:50000]\n",
        "y_test = y_test[:50000]\n",
        "\n",
        "test_labels = test_labels[:50000]\n",
        "\n",
        "X_train_raw = X_train_raw[:50000, :, :]\n",
        "X_test_raw = X_test_raw[:50000, :, :]\n",
        "\n",
        "y_train_raw = y_train_raw[:50000]\n",
        "y_test_raw = y_test_raw[:50000]\n",
        "\n",
        "test_labels_raw = test_labels_raw[:50000]"
      ],
      "metadata": {
        "id": "az28KpJYC3pG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "y_train_flat = y_train.reshape(y_train.shape[0], -1)\n",
        "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
        "y_test_flat = y_test.reshape(y_test.shape[0], -1)"
      ],
      "metadata": {
        "id": "JNNdfXsX5v9D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network Definition and Training\n",
        "In this section, the simplest neural network described in the paper is constructed and trained using the dataset processed in the previous part. This network takes in a moving window of 3 seconds. There are 32 samples per second, and every second of samples contains 32 points for each of the 3 axes, or 96 points per second. So, the input layer takes in a flattened array of 96 x 3 = 288 integers."
      ],
      "metadata": {
        "id": "r2FmW3xX43Xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the simplest network described in the paper\n",
        "with tf.device('/device:GPU:0'):\n",
        "  model_alt = Sequential()\n",
        "  model_alt.add(Input(shape=288,dtype=tf.int8, batch_size=1))\n",
        "  model_alt.add(Dense(units=512, activation='relu'))\n",
        "  model_alt.add(Dense(units=258, activation='relu'))\n",
        "  model_alt.add(Dense(units=128, activation='relu'))\n",
        "  model_alt.add(Dense(units=y_train.shape[1], activation='softmax'))\n",
        "  optimizer = tf.keras.optimizers.experimental.Adagrad(learning_rate=0.01)\n",
        "  model_alt.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ZaUO2FBDwYLA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_alt.summary()"
      ],
      "metadata": {
        "id": "eqQEe8a90VOf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f46f58-3a4a-4c8d-e275-3c22d1dfa9a5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (1, 512)                  147968    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (1, 258)                  132354    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (1, 128)                  33152     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (1, 14)                   1806      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 315,280\n",
            "Trainable params: 315,280\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The network plateaus at around 77% validation accuracy. It looks very promising, but real life performance is closer to 60%. This 60% was described in the research paper as the accuracy performance."
      ],
      "metadata": {
        "id": "xy0lCRSo5v9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  model_alt.fit(X_train_flat, y_train_flat, epochs = 30, validation_split = 0.2, batch_size = 32, verbose = 1)"
      ],
      "metadata": {
        "id": "oaT2aI6A0bFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fafaf265-778a-44db-a68f-9a52191d5820"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 10s 4ms/step - loss: 2.0929 - accuracy: 0.5164 - val_loss: 1.2586 - val_accuracy: 0.5876\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.2192 - accuracy: 0.5977 - val_loss: 1.1453 - val_accuracy: 0.6177\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.1164 - accuracy: 0.6282 - val_loss: 1.0860 - val_accuracy: 0.6400\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.0393 - accuracy: 0.6510 - val_loss: 0.9788 - val_accuracy: 0.6783\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 6s 4ms/step - loss: 0.9751 - accuracy: 0.6726 - val_loss: 0.9784 - val_accuracy: 0.6766\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.9210 - accuracy: 0.6874 - val_loss: 0.8957 - val_accuracy: 0.6901\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8738 - accuracy: 0.7015 - val_loss: 0.8643 - val_accuracy: 0.7026\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.8416 - accuracy: 0.7091 - val_loss: 0.8260 - val_accuracy: 0.7117\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.8051 - accuracy: 0.7218 - val_loss: 0.8038 - val_accuracy: 0.7230\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7796 - accuracy: 0.7308 - val_loss: 0.7776 - val_accuracy: 0.7288\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.7591 - accuracy: 0.7366 - val_loss: 0.7527 - val_accuracy: 0.7315\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.7374 - accuracy: 0.7411 - val_loss: 0.7539 - val_accuracy: 0.7308\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7198 - accuracy: 0.7477 - val_loss: 0.7355 - val_accuracy: 0.7434\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.7049 - accuracy: 0.7538 - val_loss: 0.7267 - val_accuracy: 0.7397\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.6892 - accuracy: 0.7592 - val_loss: 0.6945 - val_accuracy: 0.7536\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.6762 - accuracy: 0.7622 - val_loss: 0.7502 - val_accuracy: 0.7262\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.6656 - accuracy: 0.7643 - val_loss: 0.6843 - val_accuracy: 0.7558\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.6518 - accuracy: 0.7724 - val_loss: 0.6770 - val_accuracy: 0.7629\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.6409 - accuracy: 0.7745 - val_loss: 0.6676 - val_accuracy: 0.7628\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.6308 - accuracy: 0.7772 - val_loss: 0.6644 - val_accuracy: 0.7654\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 6s 4ms/step - loss: 0.6210 - accuracy: 0.7803 - val_loss: 0.6490 - val_accuracy: 0.7702\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.6121 - accuracy: 0.7842 - val_loss: 0.6415 - val_accuracy: 0.7789\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.6044 - accuracy: 0.7883 - val_loss: 0.6501 - val_accuracy: 0.7742\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 6s 4ms/step - loss: 0.5954 - accuracy: 0.7919 - val_loss: 0.6325 - val_accuracy: 0.7835\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.5869 - accuracy: 0.7924 - val_loss: 0.6170 - val_accuracy: 0.7791\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.5810 - accuracy: 0.7974 - val_loss: 0.6210 - val_accuracy: 0.7844\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.5748 - accuracy: 0.7986 - val_loss: 0.6180 - val_accuracy: 0.7862\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 6s 4ms/step - loss: 0.5679 - accuracy: 0.8008 - val_loss: 0.6148 - val_accuracy: 0.7894\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 6s 4ms/step - loss: 0.5602 - accuracy: 0.8048 - val_loss: 0.5985 - val_accuracy: 0.7945\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.5547 - accuracy: 0.8077 - val_loss: 0.6164 - val_accuracy: 0.7782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TensorFlow to TensorFlow Lite Conversion\n",
        "\n",
        "In this section, the model is converted from a TensorFlow model to a TFLite model that is compatible with the X-CUBE-AI toolchain. It is later written into a .tflite file that can be used to transfer the trained model onto the microcontroller."
      ],
      "metadata": {
        "id": "ILfXIyVm6MNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the simplest model (model_alt).\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "\n",
        "np_config.enable_numpy_behavior()\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_alt)\n",
        "tflite_model_quant = converter.convert()"
      ],
      "metadata": {
        "id": "rb2lud7eEFet"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
        "input_type = interpreter.get_input_details()[0]['dtype']\n",
        "print('input: ', input_type)\n",
        "output_type = interpreter.get_output_details()[0]['dtype']\n",
        "print('output: ', output_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR-wLvw1SFaU",
        "outputId": "28048f10-931c-444d-c142-b22c0514b9a9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:  <class 'numpy.int8'>\n",
            "output:  <class 'numpy.float32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open('mod_for_real.tflite', 'wb').write(tflite_model_quant)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCXLcEgySMvI",
        "outputId": "52590a3a-aa09-42fb-a92e-e5ace3366398"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1263672"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converted Model Tests\n",
        "\n",
        "In this section, a TFLite interpreter was instantiated so that the converted model could be loaded. Then, the same validation set was given to the converted data in order to see if the conversion was successful."
      ],
      "metadata": {
        "id": "eRkaqpst6ikm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to run inference on a TFLite model\n",
        "def run_tflite_model(tflite_file, test_image_indices):\n",
        "  global test_images\n",
        "\n",
        "  # Initialize the interpreter\n",
        "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "  predictions = np.zeros((len(test_image_indices),), dtype=np.int8)\n",
        "  for i, test_image_index in enumerate(test_image_indices):\n",
        "    test_image = X_test_flat[test_image_index]\n",
        "    test_label = y_test_flat[test_image_index]\n",
        "\n",
        "    # Check if the input type is quantized, then rescale input data to uint8\n",
        "\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
        "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[\"index\"])[-1]\n",
        "\n",
        "    predictions[i] = output.argmax()\n",
        "\n",
        "  return predictions\n"
      ],
      "metadata": {
        "id": "bRUCru4ZJz-8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to evaluate a TFLite model on all images\n",
        "def evaluate_model(tflite_file):\n",
        "  global test_images\n",
        "  global test_labels\n",
        "\n",
        "  test_image_indices = range(X_test_flat.shape[0])\n",
        "  predictions = run_tflite_model(tflite_file, test_image_indices)\n",
        "\n",
        "  accuracy = (np.sum(test_labels== predictions) * 100) / len(X_test)\n",
        "\n",
        "  print('Model accuracy is %.4f%% (Number of test samples=%d)' % (accuracy, len(X_test)))"
      ],
      "metadata": {
        "id": "2_XRwWzRbSNr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model('mod_for_real.tflite')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qedpQ33936r9",
        "outputId": "2bd4205c-61d8-4dad-e697-8e07d7f892c8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy is 78.3280% (Number of test samples=50000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like the conversion was successful, and the model will fit on the on-chip flash memory."
      ],
      "metadata": {
        "id": "ucKF6WIg7C8p"
      }
    }
  ]
}